{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib as jobl\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization,Input,PReLU\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "train_data = pd.read_csv('Data/gender_age_train.csv',index_col='device_id')\n",
    "test_data = pd.read_csv('Data/gender_age_test.csv',index_col='device_id')\n",
    "phone_data = pd.read_csv('Data/phone_brand_device_model.csv',encoding='utf-8')\n",
    "# Get rid of duplicate device ids in phone\n",
    "phone_data = phone_data.drop_duplicates('device_id',keep='first').set_index('device_id') \n",
    "label_categories = pd.read_csv('Data/label_categories.csv')\n",
    "app_labels = pd.read_csv('Data/app_labels.csv')\n",
    "events = pd.read_csv('Data/events.csv',parse_dates=['timestamp'], index_col='event_id')\n",
    "app_events = pd.read_csv('Data/app_events.csv',usecols=['event_id','app_id','is_active'],dtype={'is_active':bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['trainrow'] = np.arange(train_data.shape[0])\n",
    "test_data['testrow'] = np.arange(test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipeLine(train_data, test_data, phone_data, label_categories, app_labels, events, app_events):\n",
    "    print(\"Extracting features\")\n",
    "\n",
    "    print(\"\\tPhone Brands\")\n",
    "    \n",
    "    enc_brand = LabelEncoder().fit(phone_data['phone_brand'])\n",
    "    phone_data['enc_brand'] = enc_brand.transform(phone_data['phone_brand'])\n",
    "    \n",
    "    train_data['brand'] = phone_data['enc_brand']\n",
    "    test_data['brand'] = phone_data['enc_brand']\n",
    "    \n",
    "    Xtr_brand = csr_matrix((np.ones(train_data.shape[0]), \n",
    "                       (train_data.trainrow, train_data.brand)))\n",
    "    Xte_brand = csr_matrix((np.ones(test_data.shape[0]), \n",
    "                       (test_data.testrow, test_data.brand)))\n",
    "    \n",
    "    print(\"\\tDevice models\")\n",
    "\n",
    "    enc_model = LabelEncoder().fit(phone_data['device_model'])\n",
    "    phone_data['enc_model'] = enc_model.transform(phone_data['device_model'])\n",
    "\n",
    "    train_data['model'] = phone_data['enc_model']\n",
    "    test_data['model'] = phone_data['enc_model']\n",
    "\n",
    "    Xtr_model = csr_matrix((np.ones(train_data.shape[0]), \n",
    "                           (train_data.trainrow, train_data.model)))\n",
    "    Xte_model = csr_matrix((np.ones(test_data.shape[0]), \n",
    "                           (test_data.testrow, test_data.model)))\n",
    "\n",
    "    print(\"\\tApp_id\")\n",
    "\n",
    "    enc_apps = LabelEncoder().fit(app_events['app_id'])\n",
    "    app_events['enc_app'] = enc_apps.transform(app_events['app_id'])\n",
    "\n",
    "    deviceapps = (app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                           .groupby(['device_id','enc_app'])['enc_app'].agg(['size'])\n",
    "                           .merge(train_data[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                           .merge(test_data[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                           .reset_index())\n",
    "\n",
    "    napps = len(enc_apps.classes_)\n",
    "\n",
    "    d = deviceapps.dropna(subset=['trainrow'])\n",
    "    Xtr_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.enc_app)), \n",
    "                          shape=(train_data.shape[0],napps))\n",
    "    d = deviceapps.dropna(subset=['testrow'])\n",
    "    Xte_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.enc_app)), \n",
    "                          shape=(test_data.shape[0],napps))\n",
    "\n",
    "\n",
    "    print(\"\\tApp Labels\")\n",
    "\n",
    "    app_labels = app_labels.loc[app_labels.app_id.isin(app_events.app_id.unique())]\n",
    "    app_labels['enc_app'] = enc_apps.transform(app_labels.app_id)\n",
    "\n",
    "    enc_labels = LabelEncoder().fit(app_labels['label_id'])\n",
    "    app_labels['enc_label'] = enc_labels.transform(app_labels['label_id'])\n",
    "\n",
    "    nlabels = len(enc_labels.classes_)\n",
    "\n",
    "    devicelabels = (deviceapps[['device_id','enc_app']]\n",
    "                    .merge(app_labels[['enc_app','enc_label']])\n",
    "                    .groupby(['device_id','enc_label'])['enc_app'].agg(['size'])\n",
    "                    .merge(train_data[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(test_data[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "\n",
    "    d = devicelabels.dropna(subset=['trainrow'])\n",
    "    Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.enc_label)), \n",
    "                          shape=(train_data.shape[0],nlabels))\n",
    "    d = devicelabels.dropna(subset=['testrow'])\n",
    "    Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.enc_label)), \n",
    "                          shape=(test_data.shape[0],nlabels))\n",
    "\n",
    "    print(\"\\tEvent hour\")\n",
    "\n",
    "    events['hour'] = events['timestamp'].apply(lambda x : x.hour)\n",
    "    \n",
    "    hourevents = events.groupby(\"device_id\")[\"hour\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "    hourevents = hourevents.reset_index().set_index('device_id')\n",
    "    \n",
    "    train_data['event_hours'] = hourevents['hour']\n",
    "    test_data['event_hours'] = hourevents['hour']\n",
    "    \n",
    "    train_data = train_data.fillna('0')\n",
    "    test_data = test_data.fillna('0')\n",
    "    \n",
    "    vectorizer_hours = TfidfVectorizer()\n",
    "    vectorizer_hours.fit(train_data['event_hours'].values)\n",
    "\n",
    "    Xtr_hours = vectorizer_hours.transform(train_data['event_hours'].values)\n",
    "    Xte_hours = vectorizer_hours.transform(test_data['event_hours'].values)\n",
    "    \n",
    "    print(\"\\tEvent day\")\n",
    "    \n",
    "    events['dayofweek'] = events['timestamp'].apply(lambda x : x.dayofweek)\n",
    "    \n",
    "    dayevents = events.groupby(\"device_id\")[\"dayofweek\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "    dayevents = dayevents.reset_index().set_index('device_id')\n",
    "    \n",
    "    train_data['event_day'] = dayevents['dayofweek']\n",
    "    test_data['event_day'] = dayevents['dayofweek']\n",
    "    \n",
    "    train_data = train_data.fillna('0')\n",
    "    test_data = test_data.fillna('0')\n",
    "    \n",
    "    vectorizer_day = TfidfVectorizer()\n",
    "    vectorizer_day.fit(train_data['event_day'].values)\n",
    "\n",
    "    Xtr_day = vectorizer_day.transform(train_data['event_day'].values)\n",
    "    Xte_day = vectorizer_day.transform(test_data['event_day'].values)\n",
    "\n",
    "    print(\"\\tApps Active\")\n",
    "    \n",
    "    apps_active = app_events.groupby(['event_id'])['is_active'].apply(lambda x: \" \".join(str(s) for s in x)).reset_index().set_index('event_id')\n",
    "    events['apps_active'] = apps_active['is_active']\n",
    "    events_apps_active = events.groupby(\"device_id\")[\"apps_active\"].apply(lambda x: \" \".join(str(s) for s in x if str(s)!='nan'))\n",
    "    events_apps_active = events_apps_active.reset_index().set_index('device_id')\n",
    "\n",
    "    train_data['apps_active'] = events_apps_active['apps_active']\n",
    "    test_data['apps_active'] = events_apps_active['apps_active']\n",
    "\n",
    "    train_data = train_data.fillna('0')\n",
    "    test_data = test_data.fillna('0')\n",
    "\n",
    "    vectorizer_apps_active=TfidfVectorizer()\n",
    "    vectorizer_apps_active.fit(train_data['apps_active'].values)\n",
    "\n",
    "    Xtr_apps_active = vectorizer_apps_active.transform(train_data['apps_active'].values)\n",
    "    Xte_apps_active = vectorizer_apps_active.transform(test_data['apps_active'].values)\n",
    "    \n",
    "    print(\"Done with Extracting features\")\n",
    "    \n",
    "    print(\"Building classification model for Gender\")\n",
    "\n",
    "    print(\"\\tData Stacking\")\n",
    "    \n",
    "    X_train_gender = hstack((Xtr_brand,\n",
    "                      Xtr_model,\n",
    "                      Xtr_app,\n",
    "                      Xtr_label,\n",
    "                      Xtr_apps_active)).tocsr()\n",
    "\n",
    "    X_test_gender = hstack((Xte_brand,\n",
    "                      Xte_model,\n",
    "                      Xte_app,\n",
    "                      Xte_label,\n",
    "                      Xte_apps_active)).tocsr()\n",
    "    \n",
    "    targetencoder = LabelEncoder().fit(train_data.gender)\n",
    "    y_gender = targetencoder.transform(train_data.gender)\n",
    "    nclasses = len(targetencoder.classes_)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "            train_test_split(X_train_gender, y_gender, random_state=1026, test_size=0.2, stratify = y_gender)\n",
    "    \n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_val = np_utils.to_categorical(y_val)\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = 2\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.3, input_shape=(input_dim,)))\n",
    "    model.add(Dense(80))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(output_dim, init='normal', activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\tLoading weights\")\n",
    "    model.load_weights(\"best_models/best_model_gender.hdf5\")\n",
    "    \n",
    "    print(\"\\tPredicting Gender probabilites\")\n",
    "    X_train_gender = model.predict_proba(X_train_gender)\n",
    "    X_test_gender = model.predict_proba(X_test_gender)\n",
    "    \n",
    "    X_train_gender = np.argmax(X_train_gender,axis=1)\n",
    "    X_test_gender = np.argmax(X_test_gender,axis=1)\n",
    "    \n",
    "    print(\"\\tGetting Gender feature\")\n",
    "    Xtr_gender = X_train_gender.reshape(-1,1)\n",
    "    Xte_gender = X_test_gender.reshape(-1,1)\n",
    "    \n",
    "    print(\"Building Regression model for age\")\n",
    "\n",
    "    print(\"\\tData Stacking\")\n",
    "    X_train_age = hstack((Xtr_brand,\n",
    "                      Xtr_model,\n",
    "                      Xtr_app,\n",
    "                      Xtr_label,\n",
    "                      Xtr_apps_active,\n",
    "                      Xtr_hours)).tocsr()\n",
    "\n",
    "    X_test_age = hstack((Xte_brand,\n",
    "                      Xte_model,\n",
    "                      Xte_app,\n",
    "                      Xte_label,\n",
    "                      Xte_apps_active,\n",
    "                      Xte_hours)).tocsr()\n",
    "    \n",
    "    y_age = train_data.age\n",
    "    \n",
    "    print(\"\\tBuilding Linear Regression Model\")\n",
    "    reg = LinearRegression().fit(X_train_age, y_age)\n",
    "    X_train_age = reg.predict(X_train_age)\n",
    "    X_test_age = reg.predict(X_test_age)\n",
    "    \n",
    "    print(\"\\tGetting Age Feature\")\n",
    "    std_age = MinMaxScaler()\n",
    "    std_age.fit(X_train_age.reshape(-1,1))\n",
    "    Xtr_age = std_age.transform(X_train_age.reshape(-1,1))\n",
    "    Xte_age = std_age.transform(X_test_age.reshape(-1,1))\n",
    "    \n",
    "    print(\"Final Classification model for Groups\")\n",
    "    \n",
    "    print(\"\\tData Stacking\")\n",
    "    \n",
    "    X = hstack((Xtr_brand,\n",
    "                      Xtr_model,\n",
    "                      Xtr_app,\n",
    "                      Xtr_label,\n",
    "                      Xtr_hours,\n",
    "                      Xtr_day,\n",
    "                      Xtr_apps_active,\n",
    "                      Xtr_gender,\n",
    "                      Xtr_age)).tocsr()\n",
    "\n",
    "    X_test = hstack((Xte_brand,\n",
    "                      Xte_model,\n",
    "                      Xte_app,\n",
    "                      Xte_label,\n",
    "                      Xte_hours,\n",
    "                      Xte_day,\n",
    "                      Xte_apps_active,\n",
    "                      Xte_gender,\n",
    "                      Xte_age)).tocsr()\n",
    "    \n",
    "    targetencoder = LabelEncoder().fit(train_data.group)\n",
    "    y = targetencoder.transform(train_data.group)\n",
    "    nclasses = len(targetencoder.classes_)\n",
    "    \n",
    "    y = np_utils.to_categorical(y)\n",
    "    \n",
    "    print(\"\\tModel Building\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1026, test_size=0.2,stratify = y)\n",
    "    \n",
    "    print(\"\\t\\tNeural Network 1\")\n",
    "    input_shape = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_shape))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    random_seeds = [36,21,8,12,58,97,79,59,84,62,68,7,46,14,56,85,41,22,54,75]\n",
    "    \n",
    "    avg_val_loss = 0\n",
    "    \n",
    "    test_pred_avg = np.zeros((X_test.shape[0],12))\n",
    "    \n",
    "    for i in random_seeds:\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=i, test_size=0.2,stratify = y)\n",
    "        model.load_weights(\"best_models/NN1/best_model_\"+str(i)+\".hdf5\")\n",
    "        \n",
    "        predict_val = model.predict_proba(X_val)\n",
    "        val_loss=log_loss(y_val, predict_val)\n",
    "        avg_val_loss += val_loss\n",
    "        \n",
    "        pred = model.predict_proba(X_test)\n",
    "        test_pred_avg += pred\n",
    "    \n",
    "    test_pred_avg = test_pred_avg/len(random_seeds)\n",
    "    \n",
    "    avg_val_loss = avg_val_loss/len(random_seeds)\n",
    "    print(\"\\t\\tAverage Validation Log Loss for Neural Network 1 = \", avg_val_loss)\n",
    "    \n",
    "    np.save('TestPredictions/NN_1_Avg_test_prediction',test_pred_avg)\n",
    "    print(\"\\t\\tNeural Network 2\")\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.3, input_shape=(input_dim,)))\n",
    "    model.add(Dense(80))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    random_seeds = [66,23,27,76,60,75,95,50,67,77,46,78,58,97,57,29,54,93,92,10]\n",
    "    \n",
    "    avg_val_loss = 0\n",
    "    \n",
    "    test_pred_avg = np.zeros((X_test.shape[0],12))\n",
    "    \n",
    "    for i in random_seeds:\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=i, test_size=0.2,stratify = y)\n",
    "        model.load_weights(\"best_models/NN2/best_model_\"+str(i)+\".hdf5\")\n",
    "        \n",
    "        predict_val = model.predict_proba(X_val)\n",
    "        val_loss=log_loss(y_val, predict_val)\n",
    "        avg_val_loss += val_loss\n",
    "        \n",
    "        pred = model.predict_proba(X_test)\n",
    "        test_pred_avg += pred\n",
    "    \n",
    "    test_pred_avg = test_pred_avg/len(random_seeds)\n",
    "    \n",
    "    avg_val_loss = avg_val_loss/len(random_seeds)\n",
    "    print(\"\\t\\tAverage Validation Log Loss for Neural Network 2 = \", avg_val_loss)\n",
    "    \n",
    "    np.save('TestPredictions/NN_2_Avg_test_prediction',test_pred_avg)\n",
    "    \n",
    "    print(\"Model Ensembling\")\n",
    "    \n",
    "    NN1 = np.load('TestPredictions/NN_1_Avg_test_prediction.npy')\n",
    "    NN2 = np.load('TestPredictions/NN_2_Avg_test_prediction.npy')\n",
    "    \n",
    "    test_pred = 0.1*NN1+0.9*NN2\n",
    "    predict_data = pd.DataFrame(test_pred).set_index(test_data.index)\n",
    "    predict_data.columns = np.unique(train_data.group)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "\tPhone Brands\n",
      "\tDevice models\n",
      "\tApp_id\n",
      "\tApp Labels\n",
      "\tEvent hour\n",
      "\tEvent day\n",
      "\tApps Active\n",
      "Done with Extracting features\n",
      "Building classification model for Gender\n",
      "\tData Stacking\n",
      "\tLoading weights\n",
      "\tPredicting Gender probabilites\n",
      "\tGetting Gender feature\n",
      "Building Regression model for age\n",
      "\tData Stacking\n",
      "\tBuilding Linear Regression Model\n",
      "\tGetting Age Feature\n",
      "Final Classification model for Groups\n",
      "\tData Stacking\n",
      "\tModel Building\n",
      "\t\tNeural Network 1\n",
      "\t\tAverage Validation Log Loss for Neural Network 1 =  2.2394773826653918\n",
      "\t\tNeural Network 2\n",
      "\t\tAverage Validation Log Loss for Neural Network 2 =  2.2444114916397964\n",
      "Model Ensembling\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "predict_data = PipeLine(train_data, test_data, phone_data, label_categories, app_labels, events, app_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F23-</th>\n",
       "      <th>F24-26</th>\n",
       "      <th>F27-28</th>\n",
       "      <th>F29-32</th>\n",
       "      <th>F33-42</th>\n",
       "      <th>F43+</th>\n",
       "      <th>M22-</th>\n",
       "      <th>M23-26</th>\n",
       "      <th>M27-28</th>\n",
       "      <th>M29-31</th>\n",
       "      <th>M32-38</th>\n",
       "      <th>M39+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1002079943728939269</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>0.036115</td>\n",
       "      <td>0.053298</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.043226</td>\n",
       "      <td>0.124902</td>\n",
       "      <td>0.292434</td>\n",
       "      <td>0.401533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-1547860181818787117</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.071683</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.043837</td>\n",
       "      <td>0.051415</td>\n",
       "      <td>0.124199</td>\n",
       "      <td>0.275382</td>\n",
       "      <td>0.316713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7374582448058474277</td>\n",
       "      <td>0.032108</td>\n",
       "      <td>0.058115</td>\n",
       "      <td>0.061915</td>\n",
       "      <td>0.137424</td>\n",
       "      <td>0.148980</td>\n",
       "      <td>0.065997</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.058147</td>\n",
       "      <td>0.055632</td>\n",
       "      <td>0.100273</td>\n",
       "      <td>0.168393</td>\n",
       "      <td>0.095214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-6220210354783429585</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.013202</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>0.045481</td>\n",
       "      <td>0.167590</td>\n",
       "      <td>0.091289</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.219068</td>\n",
       "      <td>0.203824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-5893464122623104785</td>\n",
       "      <td>0.047229</td>\n",
       "      <td>0.056213</td>\n",
       "      <td>0.043569</td>\n",
       "      <td>0.061112</td>\n",
       "      <td>0.058987</td>\n",
       "      <td>0.044063</td>\n",
       "      <td>0.093169</td>\n",
       "      <td>0.157578</td>\n",
       "      <td>0.097888</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>0.087435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280900819321920929</td>\n",
       "      <td>0.059726</td>\n",
       "      <td>0.050846</td>\n",
       "      <td>0.037571</td>\n",
       "      <td>0.059532</td>\n",
       "      <td>0.074540</td>\n",
       "      <td>0.059818</td>\n",
       "      <td>0.098772</td>\n",
       "      <td>0.130646</td>\n",
       "      <td>0.069814</td>\n",
       "      <td>0.100470</td>\n",
       "      <td>0.131018</td>\n",
       "      <td>0.127247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818534825520551359</td>\n",
       "      <td>0.064219</td>\n",
       "      <td>0.058046</td>\n",
       "      <td>0.044965</td>\n",
       "      <td>0.065555</td>\n",
       "      <td>0.075392</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>0.102239</td>\n",
       "      <td>0.126236</td>\n",
       "      <td>0.076126</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.122903</td>\n",
       "      <td>0.105748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-8956851351560395765</td>\n",
       "      <td>0.056394</td>\n",
       "      <td>0.055516</td>\n",
       "      <td>0.040570</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.049541</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.123080</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>0.097649</td>\n",
       "      <td>0.118223</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>0.074855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6097318236795836256</td>\n",
       "      <td>0.056394</td>\n",
       "      <td>0.055516</td>\n",
       "      <td>0.040570</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.049541</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.123080</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>0.097649</td>\n",
       "      <td>0.118223</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>0.074855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622421180514002079</td>\n",
       "      <td>0.070883</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.060230</td>\n",
       "      <td>0.058526</td>\n",
       "      <td>0.046093</td>\n",
       "      <td>0.130914</td>\n",
       "      <td>0.157525</td>\n",
       "      <td>0.087111</td>\n",
       "      <td>0.098218</td>\n",
       "      <td>0.101314</td>\n",
       "      <td>0.074837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112071 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          F23-    F24-26    F27-28    F29-32    F33-42  \\\n",
       "device_id                                                                \n",
       " 1002079943728939269  0.000455  0.001848  0.003356  0.010226  0.036115   \n",
       "-1547860181818787117  0.002094  0.006524  0.009546  0.027018  0.066007   \n",
       " 7374582448058474277  0.032108  0.058115  0.061915  0.137424  0.148980   \n",
       "-6220210354783429585  0.004061  0.007716  0.006949  0.013202  0.026495   \n",
       "-5893464122623104785  0.047229  0.056213  0.043569  0.061112  0.058987   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       " 4280900819321920929  0.059726  0.050846  0.037571  0.059532  0.074540   \n",
       " 818534825520551359   0.064219  0.058046  0.044965  0.065555  0.075392   \n",
       "-8956851351560395765  0.056394  0.055516  0.040570  0.053968  0.049541   \n",
       " 6097318236795836256  0.056394  0.055516  0.040570  0.053968  0.049541   \n",
       " 622421180514002079   0.070883  0.066101  0.048247  0.060230  0.058526   \n",
       "\n",
       "                          F43+      M22-    M23-26    M27-28    M29-31  \\\n",
       "device_id                                                                \n",
       " 1002079943728939269  0.053298  0.003276  0.029331  0.043226  0.124902   \n",
       "-1547860181818787117  0.071683  0.005581  0.043837  0.051415  0.124199   \n",
       " 7374582448058474277  0.065997  0.017800  0.058147  0.055632  0.100273   \n",
       "-6220210354783429585  0.039261  0.045481  0.167590  0.091289  0.175066   \n",
       "-5893464122623104785  0.044063  0.093169  0.157578  0.097888  0.120708   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       " 4280900819321920929  0.059818  0.098772  0.130646  0.069814  0.100470   \n",
       " 818534825520551359   0.059518  0.102239  0.126236  0.076126  0.099054   \n",
       "-8956851351560395765  0.038218  0.123080  0.178429  0.097649  0.118223   \n",
       " 6097318236795836256  0.038218  0.123080  0.178429  0.097649  0.118223   \n",
       " 622421180514002079   0.046093  0.130914  0.157525  0.087111  0.098218   \n",
       "\n",
       "                        M32-38      M39+  \n",
       "device_id                                 \n",
       " 1002079943728939269  0.292434  0.401533  \n",
       "-1547860181818787117  0.275382  0.316713  \n",
       " 7374582448058474277  0.168393  0.095214  \n",
       "-6220210354783429585  0.219068  0.203824  \n",
       "-5893464122623104785  0.132048  0.087435  \n",
       "...                        ...       ...  \n",
       " 4280900819321920929  0.131018  0.127247  \n",
       " 818534825520551359   0.122903  0.105748  \n",
       "-8956851351560395765  0.113556  0.074855  \n",
       " 6097318236795836256  0.113556  0.074855  \n",
       " 622421180514002079   0.101314  0.074837  \n",
       "\n",
       "[112071 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data.to_csv('submissions_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
